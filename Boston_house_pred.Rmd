---
title: "R Notebook"
output: html_notebook
---

# preparing data
## library

```{r, message=FALSE, warning=FALSE}
library(tidymodels)
library(tidyverse)
library(janitor)
library(vip)
library(skimr)

library(xgboost)
```

## import data 

```{r}
boston <- read_csv("boston_train.csv") %>% clean_names()
kaggle <- read_csv("boston_holdout.csv") %>% clean_names()
zips   <- read_csv("zips.csv") %>% clean_names()

boston %>% skim
```
# Explorary analysis
## Histogram Target

```{r}

options(scipen = 999)
ggplot(boston, aes(x = av_total)) + 
  geom_histogram(bins = 50, col= "white") +
  labs(title=" Sale Price")+ theme(panel.grid=element_blank())

ggplot(boston, aes(x = av_total)) + 
  geom_histogram(bins = 50, col= "white") +
  scale_x_log10() +
  labs(title="Histogram Log of Sale Price") + theme(panel.grid=element_blank())

boston %>%
  filter(!yr_built == 0 ) %>%
  filter(!yr_built == 1725) %>%
  ggplot(aes(x = yr_built, y=av_total,color=av_total)) + 
  geom_point(alpha=0.5) +
  scale_color_gradient(low="lightblue", high="darkblue") +
  scale_x_continuous(limits=c(1800,2020), breaks=seq(1800,2020,10)) +
  theme(axis.text.x=element_text(angle=90,size=8))

```
## Transform 

```{r,warning=FALSE,message=FALSE}

boston_transform = boston %>%
  select(!zip )  %>%
  mutate(home_age = if_else(yr_remod > yr_built,2022 - yr_remod,2022 - yr_built))
boston_transform

kaggle_transform = kaggle %>%
  select(!zip) %>%
  mutate(home_age = if_else(yr_remod > yr_built,2022 - yr_remod,2022 - yr_built))
kaggle_transform
```

## Explore Categorical Variables 

```{r,warning=FALSE,message=FALSE}


char_col=names(boston_transform %>%
  select_if(is.character))

for(col in char_col){
  boston_transform %>%
    ggplot(aes(x=!!as.name(col), y=av_total))+
    geom_boxplot() -> p
  print(p)
}


```
## Explore Numeric Variables 


```{r,warning=FALSE,message=FALSE}

explore_numeric = skim(boston_transform %>% select_if(is.numeric)) %>%
  filter(!skim_variable %in% num_cat_col)

for (col in explore_numeric$skim_variable){
  boston_transform %>%
    ggplot(aes(x=!!as.name(col))) +
    geom_histogram(bins=30) -> graph
  print(graph)
}

```
## Correlations 
 
```{r,warning=FALSE,message=FALSE}
boston_AE = boston_transform %>%
  dplyr::select(!structure_class&!r_ac &!r_heat_typ) %>% # categorical
  dplyr::select(!pid ) %>% # numeric
  mutate_if(is.character, factor) 
skim(boston_AE)

boston_cor_string =  boston_AE %>%
  dplyr::select(is.numeric) %>%
  cor() 

corrplot::corrplot(boston_cor_string)

```

# Preparing data
## Partition our data 70/30 PLUS make K-Fold Cross Validation

Split the data 70 % train, 30% test, then make a 5 or 10 fold dataset from the test set. 

```{r}

# Save the split information for an 70/30 split of the data
bsplit <- initial_split(boston_AE, prop = 0.75)
train <- training(bsplit) 

test  <-  testing(bsplit)

# Kfold cross validation
kfold_splits <- vfold_cv(train, v=5)


```

## Recipe 

```{r}
# write out the formula 
# zipcode + land_sf+living_area + num_floors + r_total_rms + r_bdrms + r_kitch_style + r_fplace + city_state + median_income +population + pop_density
boston_recipe <-
  recipe(av_total ~ ., data = train) %>%
  step_impute_median(all_numeric_predictors()) %>% # missing values numeric 
  step_novel(all_nominal_predictors()) %>% # new factor levels 
  step_unknown(all_nominal_predictors()) %>% # missing values 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  step_nzv(all_predictors()) 

## Check the recipe results m
bake(boston_recipe %>% prep(),train %>% sample_n(1000))

```
# Linear Reg 

```{r}


lr=lm(av_total ~ ., data = train)
summary(lr)

lm_model <- linear_reg(mixture=1, penalty = 0.001) %>%
  #set_engine("lm") %>%
  set_engine("glmnet") %>%
  set_mode("regression") 

lm_wflow <-workflow() %>%
  add_recipe(boston_recipe) %>%
  add_model(lm_model) %>%
  fit(train)

tidy(lm_wflow) %>%
  mutate_if(is.numeric,round,4)

lm_wflow %>%
  pull_workflow_fit() %>%
  tidy()%>%
  mutate_if(is.numeric,round,4)

lm_wflow %>%
  pull_workflow_fit() %>%
  vi() %>% 
  mutate(Importance = if_else(Sign == "NEG", -Importance,Importance)) %>% 
  ggplot(aes(reorder(Variable,Importance),Importance, fill=Sign)) +
  geom_col() + coord_flip() + labs(title="linear model importance")
  
bind_cols(
  predict(lm_wflow,train, type="numeric"), train) %>% 
  mutate(part = "train") -> score_lm_train

bind_cols(
  predict(lm_wflow,test), test) %>% mutate(part = "test") -> score_lm_test

bind_rows(score_lm_train, score_lm_test) %>% 
  group_by(part) %>% 
  metrics(av_total,.pred) %>%
  pivot_wider(id_cols = part, names_from = .metric, values_from = .estimate)

summary(lm_model)

```

# Random Forest
```{r}

rf_model <- rand_forest(trees = 100, min_n = 20) %>%
   set_mode("regression") %>%
   set_engine("ranger", 
              num.threads = 8, 
              max.depth = 10, 
              importance="permutation")

rf_workflow <- workflow() %>%
  add_recipe(boston_recipe) %>%
  add_model(rf_model) %>%
  fit(train)

rf_workflow

bind_cols(
  predict(rf_workflow,train), train) %>% 
  metrics(av_total,.pred)

bind_cols(
  predict(rf_workflow,test), test) %>% 
  metrics(av_total,.pred)
```

# XGBoost
## XGBoost Model Buiding

Here we want to TUNE our XGB model using the Bayes method. 

```{r}

xgb_model <- boost_tree(trees=tune(), 
                        learn_rate = tune(),
                        tree_depth = tune()) %>%
  set_engine("xgboost",
             importance="permutation") %>%
  set_mode("regression")

xgb_wflow <-workflow() %>%
  add_recipe(boston_recipe) %>%
  add_model(xgb_model)

xgb_search_res <- xgb_wflow %>% 
  tune_bayes(
    resamples = kfold_splits,
    # Generate five at semi-random to start
    initial = 5,
    iter = 50, 
    # How to measure performance?
    metrics = metric_set(rmse, rsq),
    control = control_bayes(no_improve = 5, verbose = TRUE)
  )
```

## XGB Tuning 
Evaluate the tuning efforts 

```{r}
# Experiments 
xgb_search_res %>%
  collect_metrics()  %>% 
  filter(.metric == "rmse")

# Graph of learning rate 
xgb_search_res %>%
  collect_metrics() %>%
  ggplot(aes(learn_rate, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none")

# graph of tree depth 
xgb_search_res %>%
  collect_metrics() %>%
  ggplot(aes(tree_depth, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none")

# graph of number of trees 
xgb_search_res %>%
  collect_metrics() %>%
  ggplot(aes(trees, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none")
```

## Final Fit  XGB

Finally fit the XGB model using the best set of parameters 

```{r}


lowest_xgb_rmse <- xgb_search_res %>%
  select_best("rmse")

lowest_xgb_rmse

xgb_wflow <- finalize_workflow(
  xgb_wflow, lowest_xgb_rmse
) %>% 
  fit(train)

```

## VIP 
What variables are important 
```{r}
xgb_wflow %>%
  extract_fit_parsnip() %>%
  vi()

```

## Evaluate the XGBoost BEST Model 

```{r}
bind_cols(
  predict(xgb_wflow,train), train) %>% 
  metrics(av_total,.pred)

bind_cols(
  predict(xgb_wflow,test), test) %>% 
  metrics(av_total,.pred)
```
## Best Worst Predicitons 

```{r}
# best estimate 
bind_cols(predict(xgb_wflow,test),test) %>%
  mutate(error = av_total - .pred,
         abs_error = abs(error)) %>% 
  slice_min(order_by = abs_error,n=10) -> best_estimate 

best_estimate %>% 
 summarize(
    mean(error),
    mean(av_total))

# worst estimate 
bind_cols(predict(xgb_wflow,test),test)%>%
  mutate(error = av_total - .pred,
         abs_error = abs(error)) %>% 
  slice_max(order_by = abs_error,n=10) -> bottom_esimate

# overly simplistic evaluation 
overesimate %>% 
  summarize(
    mean(error),
    mean(av_total))
```

# KAGGLE 

```{r}
kaggle_sub = bind_cols(predict(xgb_wflow,kaggle_transform),kaggle_transform) %>%
  dplyr::select(pid,av_total = .pred) %>%
  write_csv("kaggle_submission_xgb2.csv")

``` 

